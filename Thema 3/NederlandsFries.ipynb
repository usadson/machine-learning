{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nederlands-Fries\n",
    "\n",
    "- https://leren.windesheim.nl/d2l/le/lessons/103162/topics/927096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vertaal Nederlandse zinnen naar het Fries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "School heeft ons al de code aangeleverd die een tekstcorpus van het Fryske Akademy downloadt, alsook deze gegevens omgezet naar een CSV-bestand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Bibliotheken importeren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (2.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (24.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (2024.9.11)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (13.7.1)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (0.3.1)\n",
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.11/dist-packages (from keras_nlp) (2.17.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_nlp) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras_nlp) (4.66.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras_nlp) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras_nlp) (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-text->keras_nlp) (2.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (70.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_nlp) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_nlp) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_nlp) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras_nlp) (2024.7.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.43.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.12.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->keras_nlp) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras_nlp\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verberg de meldingen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import keras\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "\n",
    "from keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NederlandsFries.ipynb\" not in os.listdir('.'):\n",
    "    os.chdir(\"Thema 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Inlezen `dataset.csv`\n",
    "De corpus bevat twee simpele kolommen, een met de Nederlandse tekst, en een met de Friese vertaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 173912 entries, 0 to 173911\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   nederlands  173912 non-null  object\n",
      " 1   fries       173912 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nederlands</th>\n",
       "      <th>fries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we hebben de burgemeester het advies gegeven o...</td>\n",
       "      <td>wy hawwe de boargemaster it advys jun om it ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we hebben de burgemeester het advies gegeven o...</td>\n",
       "      <td>wy hawwe de boargemaster it advys jun om it ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>een plotselinge dood</td>\n",
       "      <td>in hastige dea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>een plotselinge dood</td>\n",
       "      <td>in unferwachte dea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zijn plotseling overlijden</td>\n",
       "      <td>syn hastich ferstjerren</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          nederlands  \\\n",
       "0  we hebben de burgemeester het advies gegeven o...   \n",
       "1  we hebben de burgemeester het advies gegeven o...   \n",
       "2                               een plotselinge dood   \n",
       "3                               een plotselinge dood   \n",
       "4                         zijn plotseling overlijden   \n",
       "\n",
       "                                               fries  \n",
       "0  wy hawwe de boargemaster it advys jun om it ka...  \n",
       "1  wy hawwe de boargemaster it advys jun om it ka...  \n",
       "2                                     in hastige dea  \n",
       "3                                 in unferwachte dea  \n",
       "4                            syn hastich ferstjerren  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(\"Data/dataset.csv\")\n",
    "dataset_df.info()\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Inzicht zinlengte\n",
    "We zien dat de teksten maximaal 60 woorden bevatten, en gemiddeld 9 woorden. Gelukkig niet te veel, zo kunnen we hem makkelijker trainen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min 1 Max 59 gemiddeld 9.066044896269378\n"
     ]
    }
   ],
   "source": [
    "hoeveelheid_woorden_nederlands = dataset_df['nederlands'].apply(lambda txt: len(txt.split()))\n",
    "print(f\"Min {hoeveelheid_woorden_nederlands.min()} Max {hoeveelheid_woorden_nederlands.max()} gemiddeld {hoeveelheid_woorden_nederlands.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min 1 Max 60 gemiddeld 9.237137172823036\n"
     ]
    }
   ],
   "source": [
    "hoeveelheid_woorden_fries = dataset_df['fries'].apply(lambda txt: len(txt.split()))\n",
    "print(f\"Min {hoeveelheid_woorden_fries.min()} Max {hoeveelheid_woorden_fries.max()} gemiddeld {hoeveelheid_woorden_fries.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Inzicht woordhoeveelheid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70245"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.hstack(dataset_df['nederlands'].apply(lambda txt: np.array(txt.split())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74319"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(np.hstack(dataset_df['fries'].apply(lambda txt: np.array(txt.split())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Start en eindtokens toevoegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = \"[BOS]\"\n",
    "END_TOKEN = \"[EOS]\"\n",
    "\n",
    "def omringMetBeginEnEinde(tekst):\n",
    "    return f\"{START_TOKEN} {tekst} {END_TOKEN}\"\n",
    "\n",
    "dataset_df['fries'] = dataset_df['fries'].apply(omringMetBeginEnEinde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Splitsen tussen traindata testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nederlands</th>\n",
       "      <th>fries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101438</th>\n",
       "      <td>een vonnis bekrachtigen</td>\n",
       "      <td>[BOS] in funis befestigje [EOS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141575</th>\n",
       "      <td>de tsjechische republiek</td>\n",
       "      <td>[BOS] de servyske republyk [EOS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39299</th>\n",
       "      <td>de uitgavengroei weerspiegelde zo vooral de gr...</td>\n",
       "      <td>[BOS] de utjeftegroei wjerspegele sa foaral da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18164</th>\n",
       "      <td>nu heb hij het over voetbal terwijl hij het ov...</td>\n",
       "      <td>[BOS] no hat er it oer fuotbaljen wylst er it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3394</th>\n",
       "      <td>britten verstaat hij het best alleen is hun ac...</td>\n",
       "      <td>[BOS] britten kin er it beste ferstean allinni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               nederlands  \\\n",
       "101438                            een vonnis bekrachtigen   \n",
       "141575                           de tsjechische republiek   \n",
       "39299   de uitgavengroei weerspiegelde zo vooral de gr...   \n",
       "18164   nu heb hij het over voetbal terwijl hij het ov...   \n",
       "3394    britten verstaat hij het best alleen is hun ac...   \n",
       "\n",
       "                                                    fries  \n",
       "101438                    [BOS] in funis befestigje [EOS]  \n",
       "141575                   [BOS] de servyske republyk [EOS]  \n",
       "39299   [BOS] de utjeftegroei wjerspegele sa foaral da...  \n",
       "18164   [BOS] no hat er it oer fuotbaljen wylst er it ...  \n",
       "3394    [BOS] britten kin er it beste ferstean allinni...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dutch, test_dutch, train_frisian, test_frisian = train_test_split(\n",
    "#     dataset_df['nederlands'], dataset_df['fries'],\n",
    "#     test_size=0.2,\n",
    "#     shuffle = True,\n",
    "# )\n",
    "\n",
    "train_pairs, test_pairs = train_test_split(\n",
    "    dataset_df,\n",
    "    test_size=0.2,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "train_pairs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Tekstvectorisatie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUTCH_TOKENIZER_PATH = \"Data/dutch_tokenizer.model\"\n",
    "FRISIAN_TOKENIZER_PATH = \"Data/frisian_tokenizer.model\"\n",
    "\n",
    "# dutch_vocab_size   = 20000\n",
    "# frisian_vocab_size = 20000\n",
    "dutch_vocab_size   = 4000\n",
    "frisian_vocab_size = 4000\n",
    "\n",
    "dutch_maxlen   = 50\n",
    "frisian_maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Data/dutch_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: Data/dutch_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: Data/dutch_texts.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 173912 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=9634315\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9608% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=34\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999608\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 173912 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 173912\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 70024\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=386183 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=70269 size=20 all=1499 active=1464 piece=or\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35308 size=40 all=2497 active=2462 piece=▁en\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22696 size=60 all=3758 active=3723 piece=▁te\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16729 size=80 all=4870 active=4835 piece=id\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12187 size=100 all=6480 active=6445 piece=uw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11761 min_freq=861\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9646 size=120 all=7688 active=2134 piece=he\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7816 size=140 all=9216 active=3662 p"
     ]
    }
   ],
   "source": [
    "# train_tokenizer = not os.path.exists(DUTCH_TOKENIZER_PATH)\n",
    "train_tokenizer = True\n",
    "\n",
    "if train_tokenizer:\n",
    "    with open('Data/dutch_texts.txt', 'w') as f:\n",
    "        for line in dataset_df['nederlands']:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    # Train de tokenizer\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='Data/dutch_texts.txt',\n",
    "        model_prefix=DUTCH_TOKENIZER_PATH.split('.')[0],\n",
    "        vocab_size=dutch_vocab_size,\n",
    "        character_coverage=0.9995,  # Hiermee bereik je bijna volledige dekking van de tekst\n",
    "        model_type='bpe'  # 'bpe' is gebruikelijk voor vertaling, maar 'unigram' werkt ook goed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272, 64, 25, 2297, 1930]\n",
      "dit is een voorbeeldzin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iece=eld\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6308 size=160 all=10704 active=5150 piece=▁naar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5539 size=180 all=11934 active=6380 piece=ist\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4935 size=200 all=13093 active=7539 piece=▁ter\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4836 min_freq=745\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4450 size=220 all=14234 active=2118 piece=▁pl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4041 size=240 all=14996 active=2880 piece=▁geb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3475 size=260 all=16129 active=4013 piece=onder\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3100 size=280 all=17597 active=5481 piece=▁el\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2866 size=300 all=18399 active=6283 piece=ctie\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2854 min_freq=646\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2664 size=320 all=19151 active=1702 piece=▁bel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2491 size=340 all=20092 active=2643 piece=▁nu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2345 size=360 all=21058 active=3609 piece=pl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2169 size=380 all=22301 active=4852 piece=ort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2025 size=400 all=23450 active=6001 piece=ders\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2024 min_freq=484\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1917 size=420 all=24437 active=2101 piece=raag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1829 size=440 all=25132 active=2796 piece=▁gaat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1690 size=460 all=25921 active=3585 piece=▁zal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1563 size=480 all=26630 active=4294 piece=▁aut\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1505 size=500 all=27626 active=5290 piece=▁hele\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1504 min_freq=381\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1463 size=520 all=28622 active=2375 piece=ss\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1388 size=540 all=29484 active=3237 piece=▁belang\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1312 size=560 all=30111 active=3864 piece=▁verb\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1244 size=580 all=30472 active=4225 piece=of\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1193 size=600 all=31093 active=4846 piece=oeg\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1192 min_freq=319\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1152 size=620 all=31767 active=2192 piece=als\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1112 size=640 all=32281 active=2706 piece=achten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1083 size=660 all=32877 active=3302 piece=▁jaren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1035 size=680 all=33362 active=3787 piece=▁mis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1005 size=700 all=33938 active=4363 piece=auw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1003 min_freq=274\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=969 size=720 all=34642 active=2344 piece=nis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=929 size=740 all=35212 active=2914 piece=▁eten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=898 size=760 all=35857 active=3559 piece=▁bew\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=877 size=780 all=36731 active=4433 piece=▁natuur\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=856 size=800 all=37291 active=4993 piece=aties\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=853 min_freq=236\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=814 size=820 all=37916 active=2443 piece=gd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=785 size=840 all=38704 active=3231 piece=▁reken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=762 size=860 all=39235 active=3762 piece=▁zondag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=731 size=880 all=39721 active=4248 piece=uizen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=711 size=900 all=40138 active=4665 piece=▁kleine\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=709 min_freq=207\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=698 size=920 all=40530 active=2399 piece=lingen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=680 size=940 all=40947 active=2816 piece=▁weet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=663 size=960 all=41476 active=3345 piece=arte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=647 size=980 all=42023 active=3892 piece=▁par\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=636 size=1000 all=42492 active=4361 piece=ffen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=636 min_freq=186\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=618 size=1020 all=42954 active=2564 piece=▁maandag\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=604 size=1040 all=43393 active=3003 piece=▁5\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=592 size=1060 all=43970 active=3580 piece=▁einde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=584 size=1080 all=44522 active=4132 piece=▁anders\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=577 size=1100 all=45096 active=4706 piece=▁geweest\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=576 min_freq=166\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=564 size=1120 all=45679 active=2838 piece=▁naast\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=551 size=1140 all=46101 active=3260 piece=▁kort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=538 size=1160 all=46408 active=3567 piece=▁eigenlijk\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=529 size=1180 all=46736 active=3895 piece=▁ster\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=517 size=1200 all=47029 active=4188 piece=��mens\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=517 min_freq=153\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=508 size=1220 all=47547 active=2859 piece=▁adv\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=495 size=1240 all=48008 active=3320 piece=▁praten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=486 size=1260 all=48334 active=3646 piece=▁bekend\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=472 size=1280 all=48836 active=4148 piece=▁verhaal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=463 size=1300 all=49338 active=4650 piece=▁prijs\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=462 min_freq=141\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=453 size=1320 all=49880 active=2983 piece=jaar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=443 size=1340 all=50384 active=3487 piece=werp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=436 size=1360 all=50639 active=3742 piece=▁pat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=431 size=1380 all=51047 active=4150 piece=▁aandacht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=422 size=1400 all=51559 active=4662 piece=▁baan\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=421 min_freq=129\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=416 size=1420 all=51836 active=2829 piece=▁voort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=407 size=1440 all=52185 active=3178 piece=aring\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=400 size=1460 all=52607 active=3600 piece=schijn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=394 size=1480 all=53205 active=4198 piece=▁arbeid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=387 size=1500 all=53668 active=4661 piece=▁cr\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=387 min_freq=120\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=380 size=1520 all=53942 active=2950 piece=▁nam\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=374 size=1540 all=54208 active=3216 piece=▁veren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=368 size=1560 all=54672 active=3680 piece=matig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=363 size=1580 all=55070 active=4078 piece=wijl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=1600 all=55377 active=4385 piece=▁rest\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=356 min_freq=112\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=349 size=1620 all=55720 active=3095 piece=voud\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=344 size=1640 all=56094 active=3469 piece=▁lezen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=339 size=1660 all=56380 active=3755 piece=ute\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=335 size=1680 all=56632 active=4007 piece=▁telef\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=330 size=1700 all=56859 active=4234 piece=▁taal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=330 min_freq=106\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=1720 all=57158 active=3116 piece=oppen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=319 size=1740 all=57651 active=3609 piece=vloed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=313 size=1760 all=57986 active=3944 piece=▁zware\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=309 size=1780 all=58310 active=4268 piece=▁mond\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=306 size=1800 all=58674 active=4632 piece=▁prof\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=306 min_freq=100\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=302 size=1820 all=58921 active=3168 piece=▁leden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=298 size=1840 all=59318 active=3565 piece=▁jou\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=293 size=1860 all=59702 active=3949 piece=spron\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=289 size=1880 all=60007 active=4254 piece=opp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=284 size=1900 all=60366 active=4613 piece=▁midden\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=284 min_freq=94\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=280 size=1920 all=60665 active=3300 piece=werken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=276 size=1940 all=61024 active=3659 piece=gebied\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=273 size=1960 all=61399 active=4034 piece=blik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=269 size=1980 all=61724 active=4359 piece=mon\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=266 size=2000 all=62005 active=4640 piece=co\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=266 min_freq=88\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=264 size=2020 all=62370 active=3414 piece=▁europ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=259 size=2040 all=62624 active=3668 piece=▁12\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=2060 all=62888 active=3932 piece=▁vaste\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=254 size=2080 all=63188 active=4232 piece=▁zoek\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=250 size=2100 all=63565 active=4609 piece=kle\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=250 min_freq=83\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=247 size=2120 all=63956 active=3494 piece=▁stro\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=244 size=2140 all=64222 active=3760 piece=▁milieu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=240 size=2160 all=64550 active=4088 piece=aads\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=237 size=2180 all=64890 active=4428 piece=▁gevaar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=2200 all=65049 active=4587 piece=derd\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=234 min_freq=79\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=232 size=2220 all=65299 active=3482 piece=▁hoeft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=2240 all=65572 active=3755 piece=ude\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=227 size=2260 all=65816 active=3999 piece=▁erop\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=224 size=2280 all=66112 active=4295 piece=ield\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=222 size=2300 all=66524 active=4707 piece=▁begro\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=222 min_freq=76\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=2320 all=66743 active=3541 piece=▁vaker\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=217 size=2340 all=66956 active=3754 piece=▁zn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=214 size=2360 all=67214 active=4012 piece=atis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=212 size=2380 all=67585 active=4383 piece=▁kilom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=210 size=2400 all=67793 active=4591 piece=▁onderwerp\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=209 min_freq=73\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=207 size=2420 all=68067 active=3662 piece=gende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=205 size=2440 all=68358 active=3953 piece=appen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=203 size=2460 all=68616 active=4211 piece=▁totaal\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=200 size=2480 all=68808 active=4403 piece=ged\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=2500 all=69171 active=4766 piece=▁vertellen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=199 min_freq=70\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=196 size=2520 all=69381 active=3669 piece=▁mate\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=195 size=2540 all=69627 active=3915 piece=▁lijken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=2560 all=69909 active=4197 piece=akel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=190 size=2580 all=70138 active=4426 piece=▁bor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=188 size=2600 all=70348 active=4636 piece=azen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=188 min_freq=68\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=186 size=2620 all=70578 active=3734 piece=▁mocht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=184 size=2640 all=70789 active=3945 piece=▁ing\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=183 size=2660 all=70982 active=4138 piece=▁draa\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=2680 all=71189 active=4345 piece=▁slachtoffer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=2700 all=71407 active=4563 piece=gesteld\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=179 min_freq=65\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=177 size=2720 all=71696 active=3845 piece=▁hoop\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=2740 all=71863 active=4012 piece=▁du\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=173 size=2760 all=72206 active=4355 piece=dheid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=2780 all=72383 active=4532 piece=▁hield\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=170 size=2800 all=72571 active=4720 piece=▁dre\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=170 min_freq=62\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=169 size=2820 all=72881 active=3928 piece=▁papier\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=2840 all=73075 active=4122 piece=▁plek\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=166 size=2860 all=73233 active=4280 piece=▁maatregelen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=2880 all=73416 active=4463 piece=▁just\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=162 size=2900 all=73677 active=4724 piece=▁prim\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=162 min_freq=60\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=2920 all=73805 active=3806 piece=▁scholen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=159 size=2940 all=73949 active=3950 piece=▁ontde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=2960 all=74224 active=4225 piece=igers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=156 size=2980 all=74451 active=4452 piece=▁geluid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=3000 all=74591 active=4592 piece=behandeling\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=58\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=3020 all=74850 active=3951 piece=▁sele\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=152 size=3040 all=75001 active=4102 piece=▁volgt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=3060 all=75155 active=4256 piece=▁nodige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=150 size=3080 all=75289 active=4390 piece=▁ideeen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=3100 all=75506 active=4607 piece=▁klant\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=149 min_freq=56\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=148 size=3120 all=75688 active=3950 piece=▁stof\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=147 size=3140 all=75816 active=4078 piece=dragen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=146 size=3160 all=76118 active=4380 piece=▁bibli\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=3180 all=76249 active=4511 piece=▁natuurlijke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=3200 all=76431 active=4693 piece=perk\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=143 min_freq=54\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=142 size=3220 all=76657 active=4030 piece=▁oef\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=3240 all=76862 active=4235 piece=▁pen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=3260 all=76994 active=4367 piece=▁soorten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=3280 all=77223 active=4596 piece=▁burgemeester\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=3300 all=77445 active=4818 piece=kort\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=137 min_freq=52\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=136 size=3320 all=77558 active=3968 piece=▁cit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=3340 all=77770 active=4180 piece=▁pale\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=134 size=3360 all=77902 active=4312 piece=vinden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=3380 all=78042 active=4452 piece=▁hoek\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=3400 all=78133 active=4543 piece=ook\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=132 min_freq=51\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=3420 all=78349 active=4088 piece=▁koortsachtig\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=3440 all=78527 active=4266 piece=▁ir\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=3460 all=78714 active=4453 piece=▁zette\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=3480 all=78818 active=4557 piece=▁relatie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=127 size=3500 all=79009 active=4748 piece=vaart\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=127 min_freq=49\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=3520 all=79144 active=4050 piece=▁all\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=125 size=3540 all=79282 active=4188 piece=▁law\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3560 all=79358 active=4264 piece=▁gene\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=3580 all=79601 active=4507 piece=vlak\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=3600 all=79861 active=4767 piece=zw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=122 min_freq=48\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121 size=3620 all=80076 active=4152 piece=▁uiteen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=120 size=3640 all=80266 active=4342 piece=▁geplaatst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=3660 all=80355 active=4431 piece=ijt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=3680 all=80636 active=4712 piece=▁onderdeel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=3700 all=80806 active=4882 piece=▁wenkbrauwen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=116 min_freq=47\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=3720 all=81023 active=4258 piece=icap\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=3740 all=81137 active=4372 piece=▁kantoor\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=3760 all=81293 active=4528 piece=▁bestaande\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=3780 all=81452 active=4687 piece=▁subsid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=3800 all=81665 active=4900 piece=▁tax\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=112 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=3820 all=81805 active=4211 piece=▁partic\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=3840 all=81905 active=4311 piece=▁verlie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=3860 all=82119 active=4525 piece=▁goud\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=3880 all=82202 active=4608 piece=bre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=3900 all=82426 active=4832 piece=▁lux\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=108 min_freq=44\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=3920 all=82584 active=4275 piece=voet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=3940 all=82793 active=4484 piece=▁zinnen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=3960 all=82894 active=4585 piece=oorde\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: Data/dutch_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: Data/dutch_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "dutch_tokenizer = spm.SentencePieceProcessor(model_file=DUTCH_TOKENIZER_PATH)\n",
    "dutch_tokenizer.bos_id()\n",
    "\n",
    "def dutch_text_vectorization(input_string):\n",
    "    tokenized_sentence = dutch_tokenizer.encode(input_string, out_type=int)\n",
    "    # Vul of snijd de tokens af tot de maximale lengte\n",
    "    if len(tokenized_sentence) < dutch_maxlen:\n",
    "        tokenized_sentence += [0] * (dutch_maxlen - len(tokenized_sentence))\n",
    "    else:\n",
    "        tokenized_sentence = tokenized_sentence[:dutch_maxlen]\n",
    "    return tokenized_sentence\n",
    "\n",
    "def decode_dutch(token_ids):\n",
    "    # Filter het `END_TOKEN` als het aanwezig is\n",
    "    if END_TOKEN in token_ids:\n",
    "        token_ids = token_ids[:token_ids.index(END_TOKEN)]\n",
    "\n",
    "    # Converteer token-ID's naar tekst\n",
    "    decoded_sentence = dutch_tokenizer.decode(token_ids)\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "sample_text = \"dit is een voorbeeldzin\"\n",
    "tokenized_text = dutch_tokenizer.encode(sample_text, out_type=int)\n",
    "print(tokenized_text)  # Dit geeft de tokens in integer-vorm\n",
    "print(decode_dutch(tokenized_text))\n",
    "\n",
    "# dutch_text_vectorization = layers.TextVectorization(\n",
    "#     max_tokens=dutch_vocab_size,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=dutch_maxlen,\n",
    "# )\n",
    "# dutch_text_vectorization.adapt(dataset_df['nederlands'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: Data/frisian_texts.txt\n",
      "  input_format: \n",
      "  model_prefix: Data/frisian_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: Data/frisian_texts.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 173912 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=11270216\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9552% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=38\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999552\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 173912 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 173912\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 73976\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=347824 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=79322 size=20 all=1205 active=1166 piece=te\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=38317 size=40 all=2211 active=2172 piece=ke\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22845 size=60 all=3556 active=3517 piece=▁is\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16152 size=80 all=4722 active=4683 piece=ear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12397 size=100 all=6104 active=6065 piece=▁mo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12358 min_freq=808\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10119 size=120 all=7395 active=2251 piece=▁r\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8083 size=140 all=8713 active=3569 piece=▁dyt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6485 size=160 all=9865 active=4721 piece=ru\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5371 size=180 all=10853 active=5709 piece=iet\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4737 size=200 all=11816 active=6672 piece=eid\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4737 min_freq=738\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4290 size=220 all=12938 active=2077 piece=▁jier\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3908 size=240 all=14288 active=3427 piece=▁hiel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3449 size=260 all=15240 active=4379 piece=arre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3075 size=280 all=16306 active=5445 piece=ens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2841 size=300 all=17756 active=6895 piece=▁pl\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2831 min_freq=636\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2607 size=320 all=18678 active=1906 piece=ju\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2383 size=340 all=19798 active=3026 piece=▁si\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2228 size=360 all=20925 active=4153 piece=▁unt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2072 size=380 all=21955 active=5183 piece=▁br\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1937 size=400 all=22661 active=5889 piece=▁myn\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1936 min_freq=501\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1824 size=420 all=23687 active=2157 piece=imme\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1767 size=440 all=24166 active=2636 piece=joer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1656 size=460 all=25200 active=3670 piece=▁dei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1554 size=480 all=25956 active=4426 piece=oppe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1472 size=500 all=26953 active=5423 piece=▁sjo\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1472 min_freq=378\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1401 size=520 all=27577 active=1962 piece=rin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1346 size=540 all=28464 active=2849 piece=▁pear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1305 size=560 all=29232 active=3617 piece=▁fers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1238 size=580 all=30018 active=4403 piece=▁sint\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1193 size=600 all=30589 active=4974 piece=▁sm\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1192 min_freq=312\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1127 size=620 all=31153 active=2056 piece=gan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1095 size=640 all=31739 active=2642 piece=▁fuort\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1065 size=660 all=32528 active=3431 piece=▁wrald\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1020 size=680 all=33076 active=3979 piece=ma\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=974 size=700 all=33559 active=4462 piece=ust\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=970 min_freq=265\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=922 size=720 all=34231 active=2263 piece=▁berne\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=885 size=740 all=34919 active=2951 piece=▁19\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=861 size=760 all=35654 active=3686 piece=mei\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=837 size=780 all=36066 active=4098 piece=set\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=813 size=800 all=36688 active=4720 piece=▁rekken\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=810 min_freq=231\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=795 size=820 all=37431 active=2536 piece=▁wize\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=778 size=840 all=38158 active=3263 piece=ron\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=758 size=860 all=38821 active=3926 piece=pt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=738 size=880 all=39308 active=4413 piece=▁takom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=722 size=900 all=39738 active=4843 piece=▁grien\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=721 min_freq=201\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=709 size=920 all=40358 active=2569 piece=▁kar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=685 size=940 all=41116 active=3327 piece=fers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=664 size=960 all=41815 active=4026 piece=▁ferd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=645 size=980 all=42328 active=4539 piece=▁flu\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=628 size=1000 all=42759 active=4970 piece=▁gau\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=628 min_freq=177\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq"
     ]
    }
   ],
   "source": [
    "# train_tokenizer = not os.path.exists(FRISIAN_TOKENIZER_PATH)\n",
    "train_tokenizer = True\n",
    "\n",
    "if train_tokenizer:\n",
    "    with open('Data/frisian_texts.txt', 'w') as f:\n",
    "        for line in dataset_df['fries']:\n",
    "            f.write(line + \"\\n\")\n",
    "\n",
    "    # Train de tokenizer\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='Data/frisian_texts.txt',\n",
    "        model_prefix=FRISIAN_TOKENIZER_PATH.split('.')[0],\n",
    "        vocab_size=frisian_vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe'  # Je kunt 'bpe' of 'unigram' gebruiken afhankelijk van wat beter werkt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=609 size=1020 all=43084 active=2459 piece=keap\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=592 size=1040 all=43689 active=3064 piece=fear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=580 size=1060 all=44256 active=3631 piece=rune\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=566 size=1080 all=44720 active=4095 piece=mp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=558 size=1100 all=45212 active=4587 piece=▁besyk\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=558 min_freq=159\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=550 size=1120 all=45635 active=2678 piece=▁tiis\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=531 size=1140 all=46016 active=3059 piece=ggen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=520 size=1160 all=46599 active=3642 piece=▁haldt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=509 size=1180 all=47017 active=4060 piece=▁berik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=500 size=1200 all=47350 active=4393 piece=eske\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=500 min_freq=144\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=493 size=1220 all=47700 active=2702 piece=▁dingen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=484 size=1240 all=48174 active=3176 piece=▁makket\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=474 size=1260 all=48744 active=3746 piece=jekt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=463 size=1280 all=49039 active=4041 piece=▁go\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=454 size=1300 all=49636 active=4638 piece=▁begr\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=454 min_freq=132\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=446 size=1320 all=50066 active=2908 piece=▁holle\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=438 size=1340 all=50458 active=3300 piece=gre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=428 size=1360 all=50868 active=3710 piece=▁lin\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=422 size=1380 all=51203 active=4045 piece=▁oaren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=413 size=1400 all=51610 active=4452 piece=▁brocht\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=412 min_freq=123\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=406 size=1420 all=52135 active=3106 piece=▁sitten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=396 size=1440 all=52600 active=3571 piece=▁falt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=388 size=1460 all=52965 active=3936 piece=▁nederlanske\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=380 size=1480 all=53319 active=4290 piece=erm\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=374 size=1500 all=53833 active=4804 piece=gelyks\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=374 min_freq=113\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=369 size=1520 all=54127 active=2982 piece=▁nimmen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=363 size=1540 all=54631 active=3486 piece=▁rjochts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=356 size=1560 all=55066 active=3921 piece=ppe\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=349 size=1580 all=55417 active=4272 piece=▁datst\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=344 size=1600 all=55764 active=4619 piece=▁utst\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=344 min_freq=105\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=339 size=1620 all=56100 active=3099 piece=▁du\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=334 size=1640 all=56644 active=3643 piece=▁elekt\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=329 size=1660 all=56987 active=3986 piece=arts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=325 size=1680 all=57372 active=4371 piece=unde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=318 size=1700 all=57836 active=4835 piece=foarm\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=318 min_freq=98\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=315 size=1720 all=58154 active=3132 piece=▁stim\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=310 size=1740 all=58403 active=3381 piece=▁regel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=304 size=1760 all=58602 active=3580 piece=ugd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=299 size=1780 all=58993 active=3971 piece=▁soks\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=295 size=1800 all=59419 active=4397 piece=weint\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=295 min_freq=92\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=291 size=1820 all=59772 active=3315 piece=▁boppedat\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=286 size=1840 all=60082 active=3625 piece=sko\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=282 size=1860 all=60309 active=3852 piece=yzje\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=279 size=1880 all=60986 active=4529 piece=wez\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=276 size=1900 all=61300 active=4843 piece=▁programma\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=275 min_freq=86\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=271 size=1920 all=61772 active=3534 piece=▁ster\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=267 size=1940 all=62096 active=3858 piece=▁probleem\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=263 size=1960 all=62459 active=4221 piece=▁gewoane\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=259 size=1980 all=62859 active=4621 piece=likens\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=257 size=2000 all=63105 active=4867 piece=▁soargje\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=257 min_freq=81\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=254 size=2020 all=63365 active=3416 piece=▁gles\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=251 size=2040 all=63683 active=3734 piece=umer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=247 size=2060 all=64114 active=4165 piece=▁partijen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=245 size=2080 all=64575 active=4626 piece=▁winne\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=242 size=2100 all=64861 active=4912 piece=boek\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=242 min_freq=77\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=238 size=2120 all=65332 active=3648 piece=▁lid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=234 size=2140 all=65629 active=3945 piece=ili\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=232 size=2160 all=65913 active=4229 piece=▁suver\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=229 size=2180 all=66192 active=4508 piece=gien\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=227 size=2200 all=66518 active=4834 piece=aske\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=227 min_freq=73\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=223 size=2220 all=66875 active=3643 piece=▁fia\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220 size=2240 all=67239 active=4007 piece=sted\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=218 size=2260 all=67495 active=4263 piece=▁ferw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=216 size=2280 all=67734 active=4502 piece=▁hurde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=212 size=2300 all=68051 active=4819 piece=▁hout\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=212 min_freq=70\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=210 size=2320 all=68285 active=3616 piece=▁ungelok\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=207 size=2340 all=68589 active=3920 piece=▁top\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=204 size=2360 all=69081 active=4412 piece=hannele\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=201 size=2380 all=69321 active=4652 piece=▁siik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=199 size=2400 all=69569 active=4900 piece=egen\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=199 min_freq=67\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=197 size=2420 all=69789 active=3694 piece=▁maitiid\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=195 size=2440 all=70060 active=3965 piece=▁bestri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=192 size=2460 all=70237 active=4142 piece=▁ea\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=190 size=2480 all=70427 active=4332 piece=▁ferkearde\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=187 size=2500 all=70542 active=4447 piece=▁rige\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=187 min_freq=65\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=185 size=2520 all=70642 active=3628 piece=plan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=184 size=2540 all=71139 active=4125 piece=▁nachts\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=182 size=2560 all=71343 active=4329 piece=ons\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=181 size=2580 all=71683 active=4669 piece=▁kilometer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=179 size=2600 all=71966 active=4952 piece=▁sir\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=179 min_freq=62\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=178 size=2620 all=72104 active=3730 piece=▁hearren\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=175 size=2640 all=72429 active=4055 piece=antsje\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=173 size=2660 all=72684 active=4310 piece=gas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=172 size=2680 all=72973 active=4599 piece=▁beskriuw\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=170 size=2700 all=73342 active=4968 piece=▁april\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=170 min_freq=59\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=168 size=2720 all=73674 active=3995 piece=▁kald\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=167 size=2740 all=73841 active=4162 piece=jierrige\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=166 size=2760 all=74191 active=4512 piece=▁trochsneed\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=164 size=2780 all=74515 active=4836 piece=▁hoecht\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=162 size=2800 all=74690 active=5011 piece=bu\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=162 min_freq=57\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=161 size=2820 all=74984 active=3966 piece=jefte\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=160 size=2840 all=75366 active=4348 piece=▁fyft\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=158 size=2860 all=75600 active=4582 piece=▁gas\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=157 size=2880 all=75910 active=4892 piece=▁omstannichheden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155 size=2900 all=76061 active=5043 piece=▁hynder\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=155 min_freq=55\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=154 size=2920 all=76295 active=4036 piece=▁fjild\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=153 size=2940 all=76588 active=4329 piece=▁ienkear\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=151 size=2960 all=76745 active=4486 piece=▁et\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=150 size=2980 all=76883 active=4624 piece=▁oanbod\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=149 size=3000 all=77022 active=4763 piece=▁krisis\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=149 min_freq=53\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=147 size=3020 all=77175 active=3994 piece=iget\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=146 size=3040 all=77342 active=4161 piece=▁neg\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=145 size=3060 all=77671 active=4490 piece=jekten\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=144 size=3080 all=77895 active=4714 piece=okken\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=143 size=3100 all=78071 active=4890 piece=▁ferlyn\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=143 min_freq=51\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=141 size=3120 all=78167 active=4000 piece=▁ty\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=140 size=3140 all=78327 active=4160 piece=wan\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=139 size=3160 all=78564 active=4397 piece=▁milj\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=138 size=3180 all=78747 active=4580 piece=▁hoar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=137 size=3200 all=78880 active=4713 piece=▁underhannel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=137 min_freq=50\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=135 size=3220 all=79092 active=4153 piece=▁losse\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=134 size=3240 all=79351 active=4412 piece=▁besite\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=133 size=3260 all=79586 active=4647 piece=▁winnen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=132 size=3280 all=79696 active=4757 piece=▁kantoar\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=130 size=3300 all=79974 active=5035 piece=▁fyt\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=130 min_freq=48\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=129 size=3320 all=80045 active=4061 piece=fee\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=128 size=3340 all=80228 active=4244 piece=syd\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=127 size=3360 all=80493 active=4509 piece=▁inge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=126 size=3380 all=80597 active=4613 piece=▁mel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3400 all=80778 active=4794 piece=kw\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=124 min_freq=46\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=124 size=3420 all=81013 active=4236 piece=▁muorre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=123 size=3440 all=81096 active=4319 piece=▁untwikkeling\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=122 size=3460 all=81267 active=4490 piece=▁nijsgjirrich\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=121 size=3480 all=81466 active=4689 piece=▁spitigernoch\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=119 size=3500 all=81551 active=4774 piece=▁krus\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=119 min_freq=45\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=118 size=3520 all=81691 active=4191 piece=▁aldelju\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117 size=3540 all=81774 active=4274 piece=▁soargen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=116 size=3560 all=81897 active=4397 piece=▁rivier\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=115 size=3580 all=82050 active=4550 piece=▁region\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=114 size=3600 all=82236 active=4736 piece=oartel\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=114 min_freq=44\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=113 size=3620 all=82491 active=4355 piece=▁sied\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=3640 all=82683 active=4547 piece=amers\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=112 size=3660 all=82795 active=4659 piece=▁provinsje\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=111 size=3680 all=82956 active=4820 piece=▁skeel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=110 size=3700 all=83133 active=4997 piece=▁grif\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=110 min_freq=43\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=3720 all=83257 active=4280 piece=▁pun\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=109 size=3740 all=83426 active=4449 piece=▁presint\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=108 size=3760 all=83580 active=4603 piece=▁wiksel\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=107 size=3780 all=83766 active=4789 piece=atsoen\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=3800 all=83928 active=4951 piece=▁tip\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=106 min_freq=41\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=106 size=3820 all=84082 active=4343 piece=▁flugge\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=105 size=3840 all=84287 active=4548 piece=▁heden\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104 size=3860 all=84401 active=4662 piece=▁bekende\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=103 size=3880 all=84598 active=4859 piece=▁brie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=3900 all=84780 active=5041 piece=raal\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=102 min_freq=40\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=102 size=3920 all=84951 active=4391 piece=▁freedtemoarn\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=101 size=3940 all=85053 active=4493 piece=▁beteljen\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: Data/frisian_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: Data/frisian_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# frisian_text_vectorization = layers.TextVectorization(\n",
    "#     max_tokens=frisian_vocab_size,\n",
    "#     output_mode=\"int\",\n",
    "#     output_sequence_length=frisian_maxlen + 1, # <--- om ervoor te zorgen dat hij de volgende gaat voorspellen\n",
    "#     standardize=custom_standardization,\n",
    "# )\n",
    "# frisian_text_vectorization.adapt(dataset_df['fries'])\n",
    "\n",
    "\n",
    "frisian_tokenizer = spm.SentencePieceProcessor(model_file=FRISIAN_TOKENIZER_PATH)\n",
    "\n",
    "# Stap 3: Pas de tokenizer toe in plaats van TextVectorization\n",
    "def frisian_text_vectorization(input_string):\n",
    "    maxlen = frisian_maxlen + 1 # MOGELIJK IS DIT GEFUCKT NU :SOB:\n",
    "    tokenized_sentence = frisian_tokenizer.encode(input_string, out_type=int)\n",
    "    if len(tokenized_sentence) < maxlen:\n",
    "        tokenized_sentence += [0] * (maxlen - len(tokenized_sentence))\n",
    "    else:\n",
    "        tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "    return tokenized_sentence\n",
    "\n",
    "def decode_frisian(token_ids):\n",
    "    if END_TOKEN in token_ids:\n",
    "        token_ids = token_ids[:token_ids.index(END_TOKEN)]\n",
    "\n",
    "    # Converteer token-ID's naar tekst\n",
    "    decoded_sentence = frisian_tokenizer.decode(token_ids)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# def format_dataset(dutch: tf.Tensor, frisian):\n",
    "#     dutch = dutch.numpy().decode('utf-8')\n",
    "#     frisian = frisian.numpy().decode('utf-8')\n",
    "#     dutch = dutch_text_vectorization(dutch)\n",
    "#     frisian = frisian_text_vectorization(frisian)\n",
    "\n",
    "#     return ({\n",
    "#         \"dutch\": dutch,\n",
    "#         \"frisian\": frisian[:, :-1],\n",
    "#     }, frisian[:, 1:])\n",
    "\n",
    "def encode_text(dutch, frisian):\n",
    "    # We krijgen tensors binnen, maak er ff Python strings van\n",
    "    # dutch = dutch.numpy().astype('U13')\n",
    "    # frisian = frisian.numpy().astype('U13')\n",
    "\n",
    "    dutch_tokens = np.vectorize(dutch_text_vectorization)(dutch)\n",
    "    frisian_tokens = np.vectorize(frisian_text_vectorization)(frisian)\n",
    "\n",
    "    # Dataset werkt mn met numpy\n",
    "    return np.array(dutch_tokens, dtype=np.int32), np.array(frisian_tokens, dtype=np.int32)\n",
    "\n",
    "def format_dataset(dutch: tf.Tensor, frisian: tf.Tensor):\n",
    "    # dutch, frisian = tf.py_function(\n",
    "    #     encode_text,\n",
    "    #     inp=[dutch, frisian],\n",
    "    #     Tout=(tf.int32, tf.int32)\n",
    "    # )\n",
    "\n",
    "    print(\"Frisian\", frisian)\n",
    "\n",
    "    # Zet de output naar de juiste format\n",
    "    return ({\n",
    "        \"dutch\": dutch,\n",
    "        \"frisian\": frisian[:, :-1],  # Input voor de decoder zonder laatste token\n",
    "    }, frisian[:, 1:])  # Output met verschoven index\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    dutch_texts = pairs['nederlands']\n",
    "    frisian_texts = pairs['fries']\n",
    "    dutch_texts = list(dutch_texts)\n",
    "    frisian_texts = list(frisian_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dutch_texts, frisian_texts))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=16)\n",
    "    return dataset #.shuffle(2048).prefetch(16).cache() #in memory caching ivm performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_pairs = train_pairs.copy()\n",
    "tokenized_pairs['nederlands'] = tokenized_pairs['nederlands'].apply(dutch_text_vectorization)\n",
    "tokenized_pairs['fries'] = tokenized_pairs['fries'].apply(frisian_text_vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_pairs = test_pairs.copy()\n",
    "tokenized_test_pairs['nederlands'] = tokenized_test_pairs['nederlands'].apply(dutch_text_vectorization)\n",
    "tokenized_test_pairs['fries'] = tokenized_test_pairs['fries'].apply(frisian_text_vectorization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728844568.513526    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.521147    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.521182    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.522690    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.522731    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.522759    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.644061    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.644139    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728844568.644233    7496 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frisian Tensor(\"args_1:0\", shape=(64, 51), dtype=int32)\n",
      "Frisian Tensor(\"args_1:0\", shape=(64, 51), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "train_ds = make_dataset(tokenized_pairs)\n",
    "test_ds = make_dataset(tokenized_test_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['dutch'].shape: (64, 50)\n",
      "inputs['frisian'].shape: (64, 50)\n",
      "targets.shape: (64, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ParallelMapDataset element_spec=({'dutch': TensorSpec(shape=(64, 50), dtype=tf.int32, name=None), 'frisian': TensorSpec(shape=(64, 50), dtype=tf.int32, name=None)}, TensorSpec(shape=(64, 50), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['dutch'].shape: {inputs['dutch'].shape}\")\n",
    "    print(f\"inputs['frisian'].shape: {inputs['frisian'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dutch   = tf.data.Dataset.from_tensor_slices(tf.cast(train_dutch.values, tf.string)).batch(64)\n",
    "# test_dutch    = tf.data.Dataset.from_tensor_slices(tf.cast(test_dutch.values, tf.string)).batch(64)\n",
    "# train_frisian = tf.data.Dataset.from_tensor_slices(tf.cast(train_frisian.values, tf.string)).batch(64)\n",
    "# test_frisian  = tf.data.Dataset.from_tensor_slices(tf.cast(test_frisian.values, tf.string)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([x for x in train_dutch.take(1)][0][0])\n",
    "# print([x for x in train_frisian.take(1)][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = tf.data.Dataset.zip((train_dutch,train_frisian))\n",
    "# test_ds = tf.data.Dataset.zip((test_dutch,test_frisian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int_train_ds = train_ds.map(\n",
    "#     lambda x, y: (dutch_text_vectorization(x), frisian_text_vectorization(y)),\n",
    "#     num_parallel_calls=16)\n",
    "\n",
    "# int_test_ds = test_ds.map(\n",
    "#     lambda x, y: (dutch_text_vectorization(x), frisian_text_vectorization(y)),\n",
    "#     num_parallel_calls=16)\n",
    "\n",
    "# print(np.array([x for x in int_train_ds.take(1)]).shape)\n",
    "# print(int_train_ds)\n",
    "# # print([x for x in int_test_ds.take(1)][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Initialize the embeddings for tokens and positions\n",
    "        self.token_embeddings = layers.Embedding(           # regular embedding\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(        # position embedding\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        # Some relevant settings for subsequent layers\n",
    "        # Definnig the settings as part of the object (self.) makes it\n",
    "        # easier to apply them consistently in the call() method\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]                               # Length of the input sentences\n",
    "        positions = tf.range(start=0, limit=length, delta=1)        # 0-indexed positions of tokens in the sequences\n",
    "        # Generate the actual position embeddings\n",
    "        embedded_tokens = self.token_embeddings(inputs)             # Regular embeddings of the tokens\n",
    "        embedded_positions = self.position_embeddings(positions)    # Position embeddings\n",
    "        # We maken hier een 2e embeddingspace voor de positie die we daarna bij het origineel optellen\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return keras.ops.not_equal(inputs, 0) #geneer mask basis van waar de input niet 0 is.Zodat we de input niet hoeven te padden\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):                              # Our transformer encoder layer inherits from keras.layers.Layer\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):   # the constructor of our encoder layer\n",
    "        super().__init__(**kwargs)                                   # calls the constructor of the parent class (keras.layers.Layer)\n",
    "\n",
    "        # Store a whole bunch settings and initialise the building blocks for our encoder layer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Multi-head attention building block\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Dense building block\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # Layer normalisation building block, 1 and 2 are used to normalise the output of the attention and dense blocks respectively\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        # Here we actually build the encoder layer\n",
    "\n",
    "        # If we define a mask for attention, we need to perform some preprocessing on it\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :] #padding mask (negeer alle paffing) voeg een dimensie toe. Transformer verwacht 3D of meer. Embedding layer genereerd 2d layer\n",
    "\n",
    "        # Define the attention part of the encoder\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        # Apply layer normalisation to the attention output\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        # Apply the dense part of the encoder\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        # Apply layer normalisation to the dense output, and return the encoder\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        # Return all the configuration settings for this layer\n",
    "\n",
    "        # Get the configuration settings from the parent class\n",
    "        config = super().get_config()\n",
    "        # Add the config settings of our own layer\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True #anders geen masking mogelijk\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        #prevents the model from learning to copy the next token from the input to the output by hiden it\n",
    "        # [[1,0,0],\n",
    "        #  [1,1,0],\n",
    "        #  [1,1,1]]\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        #Replicate it along the batch axis to get an matrix of shape (batch_size, sequence_length, sequence_length)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        #retrieve the casual mask\n",
    "        padding_mask = None\n",
    "        if mask is not None: #prepare the input mask that describes padding locations in the target_sequence\n",
    "            padding_mask = keras.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\") #extra dim aangezien transfo deze verwacht\n",
    "            padding_mask = keras.minimum(padding_mask, causal_mask)#merge both masks (input padding en volgende woord padding)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)# pass the casual mask tot the first attention layer, which performs self attention over the target sequence\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask, #pass the combined mask to the second attention layer, which relates the source sequence to the target sequence\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "# disable_eager_execution()\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 64\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ter [ts](/usr/local/lib/python3.11/dist-packages/keras_nlp/src/layers/modeling/transformer_encoder.py)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.AdamW(5e-5),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    jit_compile=True,\n",
    ")\n",
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "acc=history.history['sparse_categorical_accuracy']\n",
    "val_acc=history.history['val_sparse_categorical_accuracy']\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, ziet er veelbelovend uit! Nu snel maar eens een aantal extra keer bijtrainen:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 64\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# model.compile(\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     optimizer=keras.optimizers.Adam(5e-5),\n",
    "#     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "#     jit_compile=True,\n",
    "# )\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    jit_compile=True)\n",
    "history = model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.x Meer params"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2024\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    # jit_compile=True\n",
    "    )\n",
    "history = model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.x. Lagere dropout\n",
    "Gezien overfitting geen probleem is op het moment, probeer ik hem te verlagen."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2024\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "decoder_dropout = layers.Dropout(0.1, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]    )\n",
    "history = model.fit(train_ds, epochs=5, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.x. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2024\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]    )\n",
    "history = model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#nog even testen\n",
    "import numpy as np\n",
    "import random\n",
    "fy_vocab = frisian_text_vectorization.get_vocabulary()\n",
    "fy_index_lookup = dict(zip(range(len(fy_vocab)), fy_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = dutch_text_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = frisian_text_vectorization(\n",
    "            [decoded_sentence])\n",
    "        predictions = model(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = fy_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_dutch_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_dutch_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZONDAG"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 512\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# model.compile(\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     optimizer=keras.optimizers.AdamW(5e-5),\n",
    "#     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "#     jit_compile=True,\n",
    "# )\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MASKED LOSS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "\n",
    "learning_rate = CustomSchedule(embed_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#nog even testen\n",
    "import numpy as np\n",
    "import random\n",
    "fy_vocab = frisian_text_vectorization.get_vocabulary()\n",
    "fy_index_lookup = dict(zip(range(len(fy_vocab)), fy_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = dutch_text_vectorization([input_sentence])\n",
    "    decoded_sentence = START_TOKEN\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = frisian_text_vectorization(\n",
    "            [decoded_sentence])\n",
    "        predictions = model(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = fy_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token + str(sampled_token_index)\n",
    "        if sampled_token == END_TOKEN:\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_dutch_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_dutch_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "learning_rate = CustomSchedule(embed_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#nog even testen\n",
    "import numpy as np\n",
    "import random\n",
    "fy_vocab = frisian_text_vectorization.get_vocabulary()\n",
    "fy_index_lookup = dict(zip(range(len(fy_vocab)), fy_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = dutch_text_vectorization([input_sentence])\n",
    "    decoded_sentence = START_TOKEN\n",
    "    indices = []\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence: tf.Tensor = frisian_text_vectorization([decoded_sentence])\n",
    "        tokenized_target_sentence = tokenized_target_sentence[:, :-1]\n",
    "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        # Pak altijd de laatste voorspelling\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        indices.append(sampled_token_index)\n",
    "\n",
    "        sampled_token = fy_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == END_TOKEN:\n",
    "            break\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_dutch_texts = [pair[0] for pair in test_pairs.values]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_dutch_texts)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"NL:\", input_sentence)\n",
    "    print(\"FY:\", decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "learning_rate = CustomSchedule(embed_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "history = model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "acc=history.history['masked_accuracy']\n",
    "val_acc=history.history['val_masked_accuracy']\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#nog even testen\n",
    "import numpy as np\n",
    "import random\n",
    "fy_vocab = frisian_text_vectorization.get_vocabulary()\n",
    "fy_index_lookup = dict(zip(range(len(fy_vocab)), fy_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = dutch_text_vectorization([input_sentence])\n",
    "    decoded_sentence = START_TOKEN\n",
    "    indices = []\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence: tf.Tensor = frisian_text_vectorization([decoded_sentence])\n",
    "        tokenized_target_sentence = tokenized_target_sentence[:, :-1]\n",
    "        predictions = model([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        # Pak altijd de laatste voorspelling\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        indices.append(sampled_token_index)\n",
    "\n",
    "        sampled_token = fy_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == END_TOKEN:\n",
    "            break\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "test_dutch_texts = [pair[0] for pair in test_pairs.values]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_dutch_texts)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"NL:\", input_sentence)\n",
    "    print(\"FY:\", decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPM TOKENIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'encoder_outputs' (of type TransformerEncoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ dutch (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ frisian             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">518,400</span> │ dutch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dutch[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">518,400</span> │ frisian[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbeddi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_outputs     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">791,296</span> │ encoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,319,040</span> │ decoder_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerDecode…</span> │                   │            │ encoder_outputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dropout     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_outputs     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">516,000</span> │ decoder_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ dutch (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ frisian             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m518,400\u001b[0m │ dutch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dutch[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m518,400\u001b[0m │ frisian[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mPositionalEmbeddi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_outputs     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │    \u001b[38;5;34m791,296\u001b[0m │ encoder_embeddin… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │  \u001b[38;5;34m1,319,040\u001b[0m │ decoder_embeddin… │\n",
       "│ (\u001b[38;5;33mTransformerDecode…\u001b[0m │                   │            │ encoder_outputs[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_dropout     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ decoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_outputs     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m4000\u001b[0m)  │    \u001b[38;5;34m516,000\u001b[0m │ decoder_dropout[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,663,136</span> (13.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,663,136\u001b[0m (13.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,663,136</span> (13.97 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,663,136\u001b[0m (13.97 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:915: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728844602.937020    7871 service.cc:146] XLA service 0x7f01dc007e50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728844602.937064    7871 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Ti, Compute Capability 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   2/2173\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 71ms/step - loss: 8.4152 - masked_accuracy: 0.0000e+00   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1728844615.714697    7871 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 16ms/step - loss: 5.9693 - masked_accuracy: 0.2643 - val_loss: 3.0047 - val_masked_accuracy: 0.5377\n",
      "Epoch 2/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 2.8762 - masked_accuracy: 0.5583 - val_loss: 1.9985 - val_masked_accuracy: 0.6589\n",
      "Epoch 3/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 2.0528 - masked_accuracy: 0.6673 - val_loss: 1.5101 - val_masked_accuracy: 0.7312\n",
      "Epoch 4/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.6545 - masked_accuracy: 0.7208 - val_loss: 1.3741 - val_masked_accuracy: 0.7517\n",
      "Epoch 5/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.4804 - masked_accuracy: 0.7453 - val_loss: 1.2973 - val_masked_accuracy: 0.7646\n",
      "Epoch 6/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.3669 - masked_accuracy: 0.7619 - val_loss: 1.2427 - val_masked_accuracy: 0.7753\n",
      "Epoch 7/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.2822 - masked_accuracy: 0.7750 - val_loss: 1.2060 - val_masked_accuracy: 0.7818\n",
      "Epoch 8/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.2158 - masked_accuracy: 0.7851 - val_loss: 1.1808 - val_masked_accuracy: 0.7877\n",
      "Epoch 9/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 1.1607 - masked_accuracy: 0.7937 - val_loss: 1.1612 - val_masked_accuracy: 0.7914\n",
      "Epoch 10/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 1.1154 - masked_accuracy: 0.8008 - val_loss: 1.1475 - val_masked_accuracy: 0.7954\n",
      "Epoch 11/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 14ms/step - loss: 1.0770 - masked_accuracy: 0.8069 - val_loss: 1.1353 - val_masked_accuracy: 0.7987\n",
      "Epoch 12/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.0429 - masked_accuracy: 0.8125 - val_loss: 1.1255 - val_masked_accuracy: 0.8013\n",
      "Epoch 13/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 1.0133 - masked_accuracy: 0.8173 - val_loss: 1.1241 - val_masked_accuracy: 0.8022\n",
      "Epoch 14/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 0.9862 - masked_accuracy: 0.8213 - val_loss: 1.1224 - val_masked_accuracy: 0.8037\n",
      "Epoch 15/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 15ms/step - loss: 0.9631 - masked_accuracy: 0.8251 - val_loss: 1.1196 - val_masked_accuracy: 0.8042\n",
      "Epoch 16/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.9422 - masked_accuracy: 0.8284 - val_loss: 1.1150 - val_masked_accuracy: 0.8051\n",
      "Epoch 17/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.9235 - masked_accuracy: 0.8315 - val_loss: 1.1146 - val_masked_accuracy: 0.8062\n",
      "Epoch 18/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.9046 - masked_accuracy: 0.8345 - val_loss: 1.1152 - val_masked_accuracy: 0.8071\n",
      "Epoch 19/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8875 - masked_accuracy: 0.8371 - val_loss: 1.1184 - val_masked_accuracy: 0.8076\n",
      "Epoch 20/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8715 - masked_accuracy: 0.8395 - val_loss: 1.1179 - val_masked_accuracy: 0.8082\n",
      "Epoch 21/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8583 - masked_accuracy: 0.8420 - val_loss: 1.1199 - val_masked_accuracy: 0.8085\n",
      "Epoch 22/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8439 - masked_accuracy: 0.8441 - val_loss: 1.1255 - val_masked_accuracy: 0.8089\n",
      "Epoch 23/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8304 - masked_accuracy: 0.8461 - val_loss: 1.1273 - val_masked_accuracy: 0.8092\n",
      "Epoch 24/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8193 - masked_accuracy: 0.8479 - val_loss: 1.1331 - val_masked_accuracy: 0.8094\n",
      "Epoch 25/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.8067 - masked_accuracy: 0.8500 - val_loss: 1.1294 - val_masked_accuracy: 0.8096\n",
      "Epoch 26/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.7961 - masked_accuracy: 0.8518 - val_loss: 1.1337 - val_masked_accuracy: 0.8093\n",
      "Epoch 27/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.7854 - masked_accuracy: 0.8536 - val_loss: 1.1354 - val_masked_accuracy: 0.8110\n",
      "Epoch 28/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.7768 - masked_accuracy: 0.8550 - val_loss: 1.1400 - val_masked_accuracy: 0.8102\n",
      "Epoch 29/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.7668 - masked_accuracy: 0.8562 - val_loss: 1.1471 - val_masked_accuracy: 0.8101\n",
      "Epoch 30/30\n",
      "\u001b[1m2173/2173\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - loss: 0.7578 - masked_accuracy: 0.8578 - val_loss: 1.1493 - val_masked_accuracy: 0.8098\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "dense_dim = 1024\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(dutch_maxlen,), dtype=\"int64\", name=\"dutch\")\n",
    "decoder_inputs = keras.Input(shape=(frisian_maxlen,), dtype=\"int64\", name=\"frisian\")\n",
    "\n",
    "# encoder_embedding = TokenAndPositionEmbedding(dutch_vocab_size, dutch_maxlen, embed_dim)(encoder_inputs)\n",
    "encoder_embedding = PositionalEmbedding(dutch_maxlen, dutch_vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads, name=\"encoder_outputs\")(encoder_embedding)\n",
    "\n",
    "\n",
    "# decoder_embedding = TokenAndPositionEmbedding(frisian_vocab_size, frisian_maxlen, embed_dim)(decoder_inputs)\n",
    "decoder_embedding = PositionalEmbedding(frisian_maxlen, frisian_vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
    "decoder = TransformerDecoder(embed_dim, dense_dim, num_heads, name=\"decoder\")(decoder_embedding, encoder_outputs)\n",
    "\n",
    "\n",
    "decoder_dropout = layers.Dropout(0.5, name=\"decoder_dropout\")(decoder)\n",
    "decoder_outputs = layers.Dense(frisian_vocab_size, activation=\"softmax\", name=\"decoder_outputs\")(decoder_dropout)\n",
    "\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()\n",
    "\n",
    "learning_rate = CustomSchedule(embed_dim)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "model.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "history = model.fit(train_ds, epochs=30, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0224534690>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc=history.history['masked_accuracy']\n",
    "val_acc=history.history['val_masked_accuracy']\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "NL: ik ging naar de vrouwenarts hij onderzocht me en zei dat ik in de menopauze zat\n",
      "DECODED [4, 8, 3972]\n",
      "50 is meer dan 50\n",
      "FY: [BOS]BOS] ik gie ik gie nei under nei dets de frouljusiid frouljush seiochtocht datsjesje ik siet siet yn [ [ [EOSEOSEOS]]]] [ oan oanEOS [ []]] op [\n",
      "-----------------------------------------------------------\n",
      "NL: dood bier\n",
      "DECODED [4, 8, 3972]\n",
      "50 is meer dan 50\n",
      "FY: [BOS]BOS] dea bier in [ [ kEOSEOSoek]]]] [ [ [EOSEOSEOS]]]]] [ [ [EOSEOSEOS]]]] [ [ [EOSEOSEOS]]]] [\n",
      "-----------------------------------------------------------\n",
      "NL: vanaf oktober\n",
      "DECODED [4, 8, 3972]\n",
      "50 is meer dan 50\n",
      "FY: [BOS]BOS] sunt oktober sunt [ [ isEOSEOS []]]]] [ [ [EOSEOSEOS]]]]] [ [ [EOSEOSEOS]]]]] [ [ [EOSEOSEOS]]]\n",
      "-----------------------------------------------------------\n",
      "NL: de makers beschikten niet over lichtgewicht cameras die in stof vorst of watervrije bekisting hun werk konden doen\n",
      "DECODED [4, 8, 3972]\n",
      "50 is meer dan 50\n",
      "FY: [BOS]BOS] de yn de mak de makken machtkje netich dyt oer bes foar inist wetterchten offrij de wetter bes wurk doggeist dwaan koeneing koene [ [ [EOSEOSEOS]]]]] [ [\n",
      "-----------------------------------------------------------\n",
      "NL: ook is de moslim beperkt in zijn doen en laten\n",
      "DECODED [4, 8, 3972]\n",
      "50 is meer dan 50\n",
      "FY: [BOS]BOS] it is is is ek ek ek ek beheind [ [ [EOSEOSEOS]]] ek [ [ []]EOS [ []]] [ [ [EOSEOSEOS]]]] [ [ [EOSEOS\n"
     ]
    }
   ],
   "source": [
    "#nog even testen\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence: list = dutch_tokenizer.encode(input_sentence, out_type=int)\n",
    "    while len(tokenized_input_sentence) < dutch_maxlen:\n",
    "        tokenized_input_sentence.append(0)\n",
    "\n",
    "    # tokenized_input_sentence = np.array(tokenized_input_sentence)\n",
    "\n",
    "    decoded_sentence: list = frisian_tokenizer.encode(START_TOKEN, out_type=int)\n",
    "    print(\"DECODED\", decoded_sentence)\n",
    "    indices = []\n",
    "\n",
    "    for i in range(frisian_maxlen):\n",
    "        tokenized_target_sentence = decoded_sentence.copy()\n",
    "\n",
    "        if len(tokenized_target_sentence) >= frisian_maxlen:\n",
    "            print(f\"{len(tokenized_target_sentence)} is meer dan {frisian_maxlen}\")\n",
    "            break\n",
    "\n",
    "        while len(tokenized_target_sentence) < frisian_maxlen:\n",
    "            tokenized_target_sentence.append(0)\n",
    "\n",
    "        # tokenized_target_sentence = np.array(tokenized_target_sentence)\n",
    "\n",
    "        # print(tokenized_input_sentence.shape)\n",
    "        # print(tokenized_target_sentence.shape)\n",
    "\n",
    "        # inp=np.array(tokenized_input_sentence, tokenized_target_sentence)\n",
    "        inp=[tf.constant([tokenized_input_sentence]), tf.constant([tokenized_target_sentence])]\n",
    "\n",
    "        predictions = model(inp)\n",
    "\n",
    "        # Pak altijd de laatste voorspelling\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        indices.append(sampled_token_index)\n",
    "\n",
    "        # sampled_token = fy_index_lookup[sampled_token_index]\n",
    "        # sampled_token = decode_frisian([sampled_token_index])\n",
    "        # decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        # if sampled_token == END_TOKEN:\n",
    "        #     break\n",
    "        decoded_sentence.append(int(sampled_token_index))\n",
    "\n",
    "    return frisian_tokenizer.decode_ids(decoded_sentence)\n",
    "\n",
    "test_dutch_texts = [pair[0] for pair in test_pairs.values]\n",
    "for _ in range(5):\n",
    "    input_sentence = random.choice(test_dutch_texts)\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(\"NL:\", input_sentence)\n",
    "    print(\"FY:\", decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Tekst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
