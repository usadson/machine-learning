{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d9b326",
   "metadata": {},
   "source": [
    "# Thema 3 werkcollege 1: Word Embeddings\n",
    "In dit werkcollege gaan we aan de slag met Word embeddings. Op de elo vind je een bestand `gamereviews.xlsx`met daarin 100.000 videogames reviews van Amazon (Orginenele bestand http://jmcauley.ucsd.edu/data/amazon/index_2014.html). Voor iedere revuew is het `sentiment` (1 positief; 0 negatief) bepaald op het aantal gegeven sterren (1-2 negatief; 4-5 positief) Verder bevat het bestand een kolom `reviewText` met daarin de daadwerkelijke review. We gaan proberen om op basis de text proberen te voorspellen of een review positief of negatief is. \n",
    "- lees het bestand in\n",
    "- maak een variabele `text`aan met daarin de reviews als string\n",
    "- maak een variabele `sentiment` aan met daarin een 1 als er 4 of 5 sterren gegeven zijn en een 0 als er 1 of 2 sterren gegeven zijn. Geef deze als tyoe int "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lees het bestand gamereviews in in een pandas dataframe \n",
    "# Split deze in een variabele text en een variabele sentiment met daarin de sentimenten \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb889a",
   "metadata": {},
   "source": [
    "We hebben nu 100.000 reviews, waarvan er 50.000 positief zijn. We gaan nu de tokenizen zodat ieder woord een eigen integer waarde krijgt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f664d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize de tekst ga uit van max 10.000 woorden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e8207",
   "metadata": {},
   "source": [
    "Nu hebben we eerder al gezien dat niet alle reviews even lang zijn. Daarom gaan we deze aanvullen (padding) naar 50 woorden. Lange reviews worden afgekapt. Ivm de performance beperken we ons hier tot de 1e 50 woorden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6585615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vul de reviews aan tot 50 woorden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc038e4f",
   "metadata": {},
   "source": [
    "We gaan nu sequentieel model bouwen en fitten. Het model bestaat uit een embedding layer, een flatten layer en sluit af met een Dense layer met 1 neuron. De embedding layer geven we 10.000 woorden als max en 100 dimensies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bouw een model met een embedding layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb3003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile en fit het model. epochs=10 validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856342d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot de accuracy en validated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845e4a07",
   "metadata": {},
   "source": [
    "Wat valt hier op? \n",
    "\n",
    "We testen ons model ook even met een paar eigenbedachte tweets. Schrijf hiervoor de code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe23b044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test een zelfbedachte tweet. Zorg ervoor dat er negative of positive uitkomt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d34fb",
   "metadata": {},
   "source": [
    "Vaak heb je onvoldoende data om zelf alles te trainen. Gelukkig is dit voor Engels al veelvuldig gedaan. Een bekend voorbeeld hiervan is GloVe (Global Vectors for words). We beginnen daarom met het downloaden van Glove [https://nlp.stanford.edu/projects/glove/] en download de 2014 English Wikipedia. Dit is een zipfile met daarin 100-dimensionale vectors voor 400.000 engelse woorden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download glove. Pas het path aan en run de onderstaande code\n",
    "import os\n",
    "import numpy as np\n",
    "glove_dir = 'C:\\\\Detail opzet Text\\\\GloVe\\\\'\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print(embeddings_index['the'])  #voorbeeld van een willekeurig woord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b72e0d0",
   "metadata": {},
   "source": [
    "Als het goed is zijn er nu 400000 vectors ingelezen. De volgende stap is het omzetten van de variabele embedding_index in een matrix die we aan een embedding layer kunnen voeren. Deze moet de vorm max_features, embedding_dim hebben. In dit geval dus 20.000 woorden en 100 dimensies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082dbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bestudeer, begrijp en run de onderstaande code\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_features, embedding_dim)) #initialiseren alles op 0\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_features:\n",
    "        if embedding_vector is not None:   # als het woord gevonden is\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "embedding_matrix[3] #voorbeeldje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c33ecb",
   "metadata": {},
   "source": [
    "Ons model blijft gelijk met eerder. Dus hier hoeven we niks aan te veranderen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792edba",
   "metadata": {},
   "source": [
    "We laden onze variabele embedding_matrix in als <u>weight</u> (tip: gebruik set_weights) bij de embedding layer [0].\n",
    "Aangezien we de bestaande glove matrix willen gebruiken en niet opnieuw gaan trainen zetten we trainable op False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e3629",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gebruik de embedding_matrix als weigth bij de embed_layer en set trainable of false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a400b",
   "metadata": {},
   "source": [
    "We compilen en fitten het model opnieuw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ca302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile en fit het model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot de accuracy en validated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f1c066",
   "metadata": {},
   "source": [
    "Wat valt hier op?\n",
    "\n",
    "Hieronder volgt nog een leuk stukje visualisatie van wat GloVe precies allemaal doet\n",
    "\n",
    "We beginnen met een lijstje met redelijk willekeurige woorden die we opzoeken in de GloVe dictionary. Dit geeft ons per word 50 verschilende waarden. Nu is een 50-dimensionale figuur vrij lastig te visualiseren, daarom reduceren we dit door middel van Principal Component Analysis tot 2 (zie https://en.wikipedia.org/wiki/Principal_component_analysis) en plotten we deze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af4b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from plotnine import *\n",
    "\n",
    "words = ['horse', 'cow', 'monkey', 'cat', 'apple', 'banana', 'pear', 'mango',\n",
    "         'car', 'bus', 'truck', 'bike', 'computer', 'calculator', 'keyboard', 'mouse']\n",
    "\n",
    "plot_word = pd.DataFrame(PCA(n_components=2).fit_transform([embeddings_index[w] for w in words]), \n",
    "                         columns=['PC1', 'PC2'])\n",
    "\n",
    "plot_word['word'] = words\n",
    "plot_word['type'] = list(pd.Series(['animal', 'fruit', 'transport', 'PC stuff']).repeat(4))\n",
    "\n",
    "gg = (\n",
    "    ggplot(plot_word, aes(x='PC1', y='PC2')) + \n",
    "      geom_point(aes(color='type')) + \n",
    "      geom_label(aes(label='word', color='type'))\n",
    ")\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817be1b4",
   "metadata": {},
   "source": [
    "Wat hier opvalt is dat veel vervante woorden (zoals bijvoorbeeld de voertuigen) allemaal redelijk dicht bij elkaar liggen. Wat ook opvalt is dat 'apple' tussen de fruitsoorten en de computer onderdelen is geplaatst. Hetzelde geldt ook enigszins voor 'mouse'\n",
    "\n",
    "Nu is zijn niet alleen losse woorden heel interessant, maar ook hoe woorden zich tot elkaar verhouden. Daarom plotten we in dit geval een aantal landen en hoofdsteden. wat hierbij opvalt is dat dat tussen liggende vector (verplaatsing op de x en de y-as) bij veel landen min of meer gelijk is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb2aa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_capitals = ['china', 'bejing', 'russia', 'moscow', 'japan', 'tokyo', \n",
    "                    'turkey', 'ankara', 'spain', 'madrid', 'france', 'paris',\n",
    "                    'portugal', 'lisbon', 'greece', 'athens', 'italy', 'rome',\n",
    "                    'netherlands', 'amsterdam', 'germany', 'berlin']\n",
    "plot_word = pd.DataFrame(PCA(n_components=2).fit_transform([embeddings_index[w] for w in country_capitals]), \n",
    "                         columns=['PC1', 'PC2'])\n",
    "plot_word['word'] = country_capitals\n",
    "plot_word['concept'] = pd.Series(['country', 'capital'] * int(len(plot_word) / 2))\n",
    "plot_word['country'] = list(pd.Series(['china', 'russia', 'japan', 'turkey', 'spain', 'france', 'portugal', 'greece', 'italy', 'netherlands', 'germany']).repeat(2))\n",
    "plot_word = plot_word.pivot(index='country', columns='concept')  # Needed for plotting in plotnine\n",
    "plot_word.columns = ['_'.join(col).strip() for col in plot_word.columns.values]\n",
    "\n",
    "gg = (\n",
    "    ggplot(plot_word) + \n",
    "      geom_point(aes(x='PC1_country', y='PC2_country'), color='blue') + \n",
    "      geom_text(aes(x='PC1_country', y='PC2_country', label='word_country'), ha='left') + \n",
    "      geom_point(aes(x='PC1_capital', y='PC2_capital'), color='green') + \n",
    "      geom_text(aes(x='PC1_capital', y='PC2_capital', label='word_capital'), ha='right') +\n",
    "      geom_segment(aes(x = 'PC1_country', xend='PC1_capital', y='PC2_country', yend='PC2_capital'))\n",
    ")\n",
    "gg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b7923",
   "metadata": {},
   "source": [
    "Dit gaat zelfs zo ver dat we dat je hier min-of-meer mee kunt rekenen. Hieronder zie je wat er gebeurt als je 'paris' - 'france' + 'poland' uitrekend en dan als uitkomst het dichtsbijzijnde woord pakt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79daa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Represent the embeddings as a matrix, needed for search with cKDTree\n",
    "word_indices = pd.Series(list(embeddings_index.keys()))\n",
    "cKDTree_embedding_matrix = np.array(list(embeddings_index.values()))\n",
    "\n",
    "# Perform some vector magic with embeddings\n",
    "find_capital_poland = embeddings_index['paris'] - embeddings_index['france'] + embeddings_index['poland']\n",
    "closest_word = cKDTree(cKDTree_embedding_matrix).query(find_capital_poland, k=1) #zoek de k dichtsbijzijnde neighbors op\n",
    "word_indices[closest_word[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4643e7",
   "metadata": {},
   "source": [
    "# Thema 3 werkcollege 2: Recurrent Neural Networks\n",
    "Genoeg spielerij met word_embeddings en terug naar onze videogames dataset. Nu we onze data in een bruikbaar formaat hebben kunnen we ons netwerk verder gaan opbouwen en trainen. We beginnen met het toevoegen van onze 1e SimpleRNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6538e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vervang de flatten layer met een SimpleRNN layer met 32 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile en fit epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot de accuracy en validated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caf50dd",
   "metadata": {},
   "source": [
    "Wat valt hier op?\n",
    "\n",
    "Eventueel kun je het netwerk nog wat dieper trainer door er 1 of meerdere extra SimpleRNN layers aan toe te voegen. Gezien de tijd die het trainen van dit netwerk zou kosten gaan we dat nu even niet doen. Mocht je het wel thuis willen proberen, let er dan op dat je de juiste output 'returned'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f4055",
   "metadata": {},
   "source": [
    "Nu is een SimpleRNN eigenlijk wat te simpel om de complexiteit van tekst te vatten, daarom vervangen we onze simpleRNN_layer door een LSTM layer met voor het gemak ook maar 32 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vervang de simpleRNN_layer door een LSTM_layer met 32 neurons epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2095a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot de accuracy en validated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf98069",
   "metadata": {},
   "source": [
    "Wat valt hier op?\n",
    "\n",
    "We proberen het nog eens met een 2e LSTM-laag ook met 32 neurons (let weer op de return sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4ea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#voeg een 2e LSTM laag toe, compile, fit en plot de (validated) accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7242af",
   "metadata": {},
   "source": [
    "Voegt dit veel toe? \n",
    "\n",
    "Een alternatief voor LSTM-layers zijn GRU-layer, vervang daarom de LSTM-layers door GRU-layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vervang de LSTM-layers door GRU-layers, compile, fit en plot de (validated) accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc356093",
   "metadata": {},
   "source": [
    "Wat zijn de verschillen t.o.v. ons model met LSTM layer? \n",
    "\n",
    "Aangezien ons model nog steeds overfit voegen we aan de 1e GRU-layer een dropout en een recurrent dropout van 0.2 toe. Ivm performance beperken we ons tot 2 epochs (het punt waar de vorige ging overfitten) en forceren we het model op de cpu te draaien. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fc37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voeg aan de 1e GRU layer een dropout en recurrentdropout van 0.2 toe, compile fit en plot\n",
    "#NB: beperkt je ivm de performance tot 2 epochs en forceer op cpu\n",
    "# with tf.device('/cpu:0'):\n",
    "#    history = model.fit(.....)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016a057",
   "metadata": {},
   "source": [
    "Wat zien we gebeuren?\n",
    "\n",
    "Een andere optie om te verkennen is het bi-directioneel maken van 1 of beide GRU layers. Ivm de performance schrappen we de recurrent_dropout (de dropout blijft)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3325a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#maak beide GRU-layers bi-directional, schrap de recurrent_dropout. Compile, fit (10 epochs) en plot de (validated) accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff06d57",
   "metadata": {},
   "source": [
    "Hoeveel effect heeft dit?\n",
    "\n",
    "Tot slot kijken we naar het effect als we onze GRU-layers vervangen door conv1d_layers, met bijbehorende maxpooling 1d en flatten layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vervang beide GRU layers door Conv1D_layers, een MaxPooling1d_layer, en een flatten_layer. Compile, Fit en Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56105cae",
   "metadata": {},
   "source": [
    "Hoeveel effect heeft dit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26199269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f8eb10",
   "metadata": {},
   "source": [
    "Wat kun je zeggen over het gebruik van RNN-layers voor sentiment-analyse?\n",
    "\n",
    "In dit geval lijken de RNN layers een beperkt effect te hebben. Bij heel kleine datasets is het zo dat een bag-of-words benadering zoals met alleen een embedding het vaak beter doen RNN's. De belangrijkste oorzaak zit hem in <u>de hoeveelheid samples t.o.v. de gemiddelde sample lengte </u> (hier afgekapt op 100).  Een richtlijn uit cholet p349 is dat als het <u> aantal samples gedeeld door de gemiddelde sample length < 1500</u> een bag-of-words methode beter werkt dan een transformer of een RNN. In deze casus trainen we met 90.000 reviews en 50 woorden = 1800 waardoor we nog maar heel beperkt boven deze vuistregel zitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d688ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
