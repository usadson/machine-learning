{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boekenclassificatie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aan jou de taak om een neuraal netwerk te bouwen dat zo goed mogelijk kan voorspellen in welk genre een sample van een boek zich bevind. Laat met een cross validation matrix zien hoe goed je model zich gedraagt per genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data van [Project Gutenberg](https://www.gutenberg.org/)\n",
    "- [Bibliotheek van Congress-classificatie](https://www.loc.gov/catdir/cpso/lcc.html)\n",
    "\n",
    "## 1.1. Aangeleverde data\n",
    "\n",
    "\n",
    "## 1.2. Genres\n",
    "De boeken zijn ingedeeld onder de volgende zeven genres:\n",
    "|Genre                                              |Engelse Vertaling                              |Aantal boeken\n",
    "|---------------------------------------------------|-----------------------------------------------|-------------\n",
    "|Amerikaanse Literatuur                             |American Literature                            |4480\n",
    "|Engelse Literatuur                                 |English Literature                             |4214\n",
    "|Fictie en jeugdliteratuur                          |Fiction and juvenile belles lettres            |2624\n",
    "|Geschiedenis van Europa, Azië, Afrika en Oceanië   |History of Europa, Asia, Africa and Oceania    |2071\n",
    "|Filosofie, Psychologie en Religie                  |Philosophy, Psychology and Religion            |1550\n",
    "|Taal en Literatuur                                 |Language and Literature                        |1264\n",
    "|Tijdschriften                                      |Periodicals                                    |1132"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Importeren van benodigde pakketten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Inspecteren van `training.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = pd.read_csv(\"Data/huiswerk/training.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om een eerste blik te werpen op de traindata, gebruik ik de volgende methodes. Ik zie dat er zo’n 120 duizend regels zijn, elk met:\n",
    "- Een `usage`-veld, waarschijnlijk voor `training.csv` alleen maar `train`, voor `testing.csv` alleen maar `testing`, etc.\n",
    "- Een `main genre`-veld, dat waarschijnlijk overeenkomt met de genres uit de Business Understanding\n",
    "- Een `samplenumber`-veld, elk boek is gesplitst per 600 woorden, deze geeft aan wat eindpunt is van de genomen sample is\n",
    "- Een `txt`-veld, met maximaal 600 woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.info()\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In verband met performance, pak ik eerst 20 duizend items, in plaats van de totale hoeveelheid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df = training_df.head(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. `train`-veld\n",
    "Het klopt inderdaad dat dit bestand alleen `train` als `usage` bevat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_usages = [[x] for x in training_df['usage'].unique()]\n",
    "pd.DataFrame(unique_usages, columns=['Usage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. `main genre`-veld\n",
    "Om te testen of de genres overeenkomen met de zeven uit de Business Understanding, pak ik de unieke genres van de dataframe, en zie dat ze inderdaad overeenkomen met de juiste uit de Business Understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unieke_genres = training_df['main genre'].unique()\n",
    "pd.DataFrame({'Genre': unieke_genres})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3. `txt`-veld\n",
    "Het `txt`-veld bevat een deel van de tekst van een boek. Elk boek is namelijk gesplitst per 600 woorden, dus het veld bevat een deel van het boek. Zoals je je hieronder kunt zien, slaan de vijf zinnen los nergens op (althans het einde ervan), maar als je dan verderleest op de volgende regel, zul je zien dat het wél klopt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in training_df['txt'].head():\n",
    "    print('TXT: ' + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om een beeld te krijgen van het aantal tekens in het veld, pak ik de standaardafwijking, het minimum en maximum. Hier zie ik dat er een verschoven verdeling zit in het aantal tekens per `txt` veld:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_lengtes = training_df['txt'].apply(lambda txt: len(txt))\n",
    "print(f\"Standaardafwijking van het `txt`-veld is: {txt_lengtes.std()} tekens\")\n",
    "print(f\"Gemiddelde van het `txt`-veld is: {txt_lengtes.mean()} tekens\")\n",
    "print(f\"Met een minimum van {txt_lengtes.min()} teken(s)\")\n",
    "print(f\"En een maximum van {txt_lengtes.max()} tekens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Omdat het aantal tekens wat verschoven is, vind ik het interessant om te kijken naar het aantal woorden per `txt`, waar ik zie dat het aantal woorden inderdaad rond de 600 zit, maar er toch een aantal regels zijn die niet zoveel bevatten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df['woordaantal'] = training_df['txt'].apply(lambda txt: len(txt.split()))\n",
    "txt_lengtes = training_df['woordaantal']\n",
    "print(f\"Standaardafwijking van het `txt`-veld is: {txt_lengtes.std()} woorden\")\n",
    "print(f\"Gemiddelde van het `txt`-veld is: {txt_lengtes.mean()} tekens\")\n",
    "print(f\"Met een minimum van {txt_lengtes.min()} woord(en)\")\n",
    "print(f\"En een maximum van {txt_lengtes.max()} woorden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, het lijkt erop dat er een aantal regels zijn met weinig woorden, maar hoeveel zijn dat er? Hieronder zien we dat het grootste deel wel 600 woorden lang is, en maar $0.07 \\%$ daaronder zit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = []\n",
    "percentages = []\n",
    "counts = []\n",
    "for x in txt_lengtes.groupby(txt_lengtes // 100):\n",
    "    percentage = (len(x[1]) / len(txt_lengtes) * 100)\n",
    "    ranges.append(f'{x[0]}00-{x[0]+1}00')\n",
    "    percentages.append(f\"{percentage:2.2f}%\")\n",
    "    counts.append(len(x[1]))\n",
    "\n",
    "pd.DataFrame({'Reeksen': ranges, 'Percentages': percentages, 'Aantallen': counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de tokenizer te kunnen laten werken in het model, is het belangrijk om het aantal verschillende woorden ongeveer te weten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = set(itertools.chain.from_iterable(training_df['txt'].apply(lambda txt: txt.split())))\n",
    "len(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Inladen van `testing.csv` en `validation.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv(\"Data/huiswerk/testing.csv.gz\")\n",
    "testing_df = testing_df.head(20000)\n",
    "testing_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv(\"Data/huiswerk/validation.csv.gz\")\n",
    "validation_df = validation_df.head(20000)\n",
    "validation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df['woordaantal'] = testing_df['txt'].apply(lambda txt: len(txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df['woordaantal'] = validation_df['txt'].apply(lambda txt: len(txt.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Filteren van teksten onder de 600 woorden\n",
    "Zoals we zagen bij de Data Understanding, zijn de meeste teksten precies 600 woorden. Alles wat daaronder zit is een miniscule hoeveelheid, en filter ik daarom gewoon weg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AANTAL_WOORDEN_PER_TEKST = 600\n",
    "training_df = training_df[training_df['woordaantal'] == AANTAL_WOORDEN_PER_TEKST]\n",
    "training_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = testing_df[testing_df['woordaantal'] == AANTAL_WOORDEN_PER_TEKST]\n",
    "validation_df = validation_df[validation_df['woordaantal'] == AANTAL_WOORDEN_PER_TEKST]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Omzetten labels\n",
    "Om te kunnen werken met getallen bij de labels, gebruik ik de [`LabelEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) van _scikit-learn_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(unieke_genres)\n",
    "LABEL_COUNT = len(unieke_genres)\n",
    "\n",
    "train_labels = le.transform(training_df['main genre'])\n",
    "testing_labels = le.transform(testing_df['main genre'])\n",
    "validation_labels = le.transform(validation_df['main genre'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Opzetten tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "AANTAL_WOORDEN = 200000\n",
    "tokenizer = Tokenizer(num_words=AANTAL_WOORDEN)\n",
    "tokenizer.fit_on_texts(training_df['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tokenizer.word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(training_df['txt'])\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_df['txt'])\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_df['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sequences\n",
    "del percentage\n",
    "del percentages\n",
    "del ranges\n",
    "del txt_lengtes\n",
    "del unieke_genres\n",
    "del unique_usages\n",
    "del x\n",
    "del testing_df\n",
    "del tmp\n",
    "del training_df\n",
    "del validation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS=100\n",
    "model = models.Sequential([\n",
    "    layers.Embedding(input_dim=AANTAL_WOORDEN, output_dim=DIMS, input_length=AANTAL_WOORDEN_PER_TEKST),\n",
    "    layers.Bidirectional(layers.GRU(32, return_sequences=True, dropout=0.2)),\n",
    "    layers.Bidirectional(layers.GRU(32)),\n",
    "    layers.Dense(LABEL_COUNT, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    history2 = model.fit(train_sequences,\n",
    "                         train_labels,\n",
    "                        #  validation_data=(validation_sequences, validation_labels),\n",
    "                         epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot de accuracy en validated accuracy\n",
    "acc=history.history['accuracy']\n",
    "val_acc=history.history['val_accuracy']\n",
    "epochs=range(1, len(acc)+1)\n",
    "plt.plot(epochs, acc, label='accuracy')\n",
    "plt.plot(epochs, val_acc, label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
